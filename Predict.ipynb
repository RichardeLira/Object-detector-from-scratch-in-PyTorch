{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Config\n",
    "from torchvision import transforms\n",
    "import mimetypes\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Drawing Inference from the Object Detector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -i INPUT\n",
      "ipykernel_launcher.py: error: the following arguments are required: -i/--input\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# construct the argument parser and parse the arguments \n",
    "\n",
    "from argparse import FileType\n",
    "\n",
    "\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--input\", required=True,\n",
    "\thelp=r\"C:\\Users\\richa\\OneDrive\\Documentos\\Object-detector-from-scratch-in-PyTorch\\plane.jpg\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# determine the input file type, but assume that we are working with single input image\n",
    "\n",
    "filetype = mimetypes.guess_type(args[\"input\"])\n",
    "imagePaths = [args[\"input\"]]\n",
    "\n",
    "\n",
    "if \"text/plain\" == filetype: \n",
    "    # load the image paths in our testing file \n",
    "    imagePaths = open(args[\"input\"]).read().strip().split(\"\\n\")\n",
    "\n",
    "# load our object detector, set it evaluation mode, and label \n",
    "# encoder from disk \n",
    "\n",
    "print(\"[INFO] : Loading object detector...\")\n",
    "model = torch.load(Config.MODEL_PATH).to(Config.DEVICE)\n",
    "model.eval()\n",
    "\n",
    "le = pickle.loads(open(Config.LE_PATH, \"rb\").read())\n",
    "\n",
    "# define normalization transforms \n",
    "transforms = transforms.Compose(\n",
    "    [transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean= Config.MEAN, std=Config.STD)]\n",
    ")\n",
    "\n",
    "\n",
    "# loop over the images that we'll be testing using our bounding box\n",
    "# regression model\n",
    "for imagePath in imagePaths:\n",
    "\t# load the image, copy it, swap its colors channels, resize it, and\n",
    "\t# bring its channel dimension forward\n",
    "\timage = cv2.imread(imagePath)\n",
    "\torig = image.copy()\n",
    "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\timage = cv2.resize(image, (224, 224))\n",
    "\timage = image.transpose((2, 0, 1))\n",
    "\t# convert image to PyTorch tensor, normalize it, flash it to the\n",
    "\t# current device, and add a batch dimension\n",
    "\timage = torch.from_numpy(image)\n",
    "\timage = transforms(image).to(Config.DEVICE)\n",
    "\timage = image.unsqueeze(0)\n",
    "\n",
    "\n",
    "\t# predict the bounding box of the object along with the class\n",
    "\t# label\n",
    "\t(boxPreds, labelPreds) = model(image)\n",
    "\t(startX, startY, endX, endY) = boxPreds[0]\n",
    "\t# determine the class label with the largest predicted\n",
    "\t# probability\n",
    "\tlabelPreds = torch.nn.Softmax(dim=-1)(labelPreds)\n",
    "\ti = labelPreds.argmax(dim=-1).cpu()\n",
    "\tlabel = le.inverse_transform(i)[0]\n",
    "\n",
    " \t# predict the bounding box of the object along with the class\n",
    "\t# label\n",
    "\t(boxPreds, labelPreds) = model(image)\n",
    "\t(startX, startY, endX, endY) = boxPreds[0]\n",
    "\t# determine the class label with the largest predicted\n",
    "\t# probability\n",
    "\tlabelPreds = torch.nn.Softmax(dim=-1)(labelPreds)\n",
    "\ti = labelPreds.argmax(dim=-1).cpu()\n",
    "\tlabel = le.inverse_transform(i)[0]\n",
    "\n",
    "    # resize the original image such that it fits on our screen, and\n",
    "\t# grab its dimensions\n",
    "\torig = imutils.resize(orig, width=600)\n",
    "\t(h, w) = orig.shape[:2]\n",
    "\t# scale the predicted bounding box coordinates based on the image\n",
    "\t# dimensions\n",
    "\tstartX = int(startX * w)\n",
    "\tstartY = int(startY * h)\n",
    "\tendX = int(endX * w)\n",
    "\tendY = int(endY * h)\n",
    "\t# draw the predicted bounding box and class label on the image\n",
    "\ty = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "\tcv2.putText(orig, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "\t\t0.65, (0, 255, 0), 2)\n",
    "\tcv2.rectangle(orig, (startX, startY), (endX, endY),\n",
    "\t\t(0, 255, 0), 2)\n",
    "\t# show the output image \n",
    "\tcv2.imshow(\"Output\", orig)\n",
    "\tcv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a7892181bc6a8b5934ec9e9bbb9c30c52104e5f153701f05479b274bedc3f5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
